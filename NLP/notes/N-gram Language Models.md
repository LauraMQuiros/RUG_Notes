## Generating from a language model

Given a bigram model, how to generate a sequence? $P(w_1,w_2,... w_t)= \Pi^t_{i=1} P(w_i | w_{i-1})$ 
We can sample the first word $w_1 \sim P(w)$ 

Text generated by a corpus will be grammatically correct, but it won't be coherent!
backoff compute over bigrams instead of trigrams (one less)

Laplace smoothing: add size of vocab |V| and alpha = 1 such that all the entries are added one. We normalise frequencies

Linear interpolation: we add up diff models to estimate p_i

Neural Language Models ($\mathbb{R}^d \rightarrow \mathbb{R}^{5d} \rightarrow \mathbb{R}^h$)
$m$: the number of words being used from the past (input)
$d$: word embedding size
$h$: hidden size

W linearly scales with context size
input layer (m) $x= [e(the), e(cat), e(sat), e(on), e(the)] \in \mathbb{R}^{md}$
hidden layer $h= tanh (Wx + b) \in \mathbb{R}^h$ 
output layer $Uh \in \mathbb{R}^{|V|}$ 